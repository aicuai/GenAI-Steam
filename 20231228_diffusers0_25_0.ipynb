{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aicuai/GenAI-Steam/blob/main/20231228_diffusers0_25_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# diffusers-0.25.0の機能テスト\n",
        "\n",
        "T4 GPU以上で動きます\n",
        "\n",
        "https://github.com/huggingface/diffusers/releases/tag/v0.25.0\n",
        "\n",
        "https://note.com/o_ob/n/nf7869b5974e4\n",
        "\n"
      ],
      "metadata": {
        "id": "kGvJ9UffMnao"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# aMUSEd\n",
        "\n",
        "aMUSEd は、MUSE アーキテクチャに基づいた軽量の *TextToImageモデル* です。\n",
        "aMUSEd は、一度に多くの画像を素早く生成するなど、軽量で高速なモデルを必要とするアプリケーションで特に役立ちます。 aMUSEd は現在研究リリースです。\n",
        "\n",
        "aMUSEd は、多くの diffusion モデルよりも少ない順方向パスでイメージを生成できる VQVAE トークンベースのトランスフォーマーです。 MUSE とは対照的に、T5-XXL の代わりに小型のテキスト エンコーダー CLIP-L/14 を使用します。 ammused はパラメータ数が少なく、フォワード パス生成プロセスが少ないため、多くの画像を迅速に生成できます。 この利点は、特に大きなバッチサイズで見られます。\n",
        "\n",
        "## Text-to-image generation\n",
        "\n",
        "```\n",
        "import torch\n",
        "from diffusers import AmusedPipeline\n",
        "\n",
        "pipe = AmusedPipeline.from_pretrained(\n",
        "    \"amused/amused-512\", variant=\"fp16\", torch_dtype=torch.float16\n",
        ")\n",
        "pipe.vqvae.to(torch.float32)  # vqvae is producing nans in fp16\n",
        "pipe = pipe.to(\"cuda\")\n",
        "\n",
        "prompt = \"cowboy\"\n",
        "image = pipe(prompt, generator=torch.manual_seed(8)).images[0]\n",
        "image.save(\"text2image_512.png\")\n",
        "Image-to-image generation\n",
        "\n",
        "import torch\n",
        "from diffusers import AmusedImg2ImgPipeline\n",
        "from diffusers.utils import load_image\n",
        "\n",
        "pipe = AmusedImg2ImgPipeline.from_pretrained(\n",
        "    \"amused/amused-512\", variant=\"fp16\", torch_dtype=torch.float16\n",
        ")\n",
        "pipe.vqvae.to(torch.float32)  # vqvae is producing nans in fp16\n",
        "pipe = pipe.to(\"cuda\")\n",
        "\n",
        "prompt = \"apple watercolor\"\n",
        "input_image = (\n",
        "    load_image(\n",
        "        \"https://raw.githubusercontent.com/huggingface/amused/main/assets/image2image_256_orig.png\"\n",
        "    )\n",
        "    .resize((512, 512))\n",
        "    .convert(\"RGB\")\n",
        ")\n",
        "\n",
        "image = pipe(prompt, input_image, strength=0.7, generator=torch.manual_seed(3)).images[0]\n",
        "image.save(\"image2image_512.png\")\n",
        "```\n",
        "\n",
        "## Inpainting\n",
        "```\n",
        "import torch\n",
        "from diffusers import AmusedInpaintPipeline\n",
        "from diffusers.utils import load_image\n",
        "from PIL import Image\n",
        "\n",
        "pipe = AmusedInpaintPipeline.from_pretrained(\n",
        "    \"amused/amused-512\", variant=\"fp16\", torch_dtype=torch.float16\n",
        ")\n",
        "pipe.vqvae.to(torch.float32)  # vqvae is producing nans in fp16\n",
        "pipe = pipe.to(\"cuda\")\n",
        "\n",
        "prompt = \"a man with glasses\"\n",
        "input_image = (\n",
        "    load_image(\n",
        "        \"https://raw.githubusercontent.com/huggingface/amused/main/assets/inpainting_256_orig.png\"\n",
        "    )\n",
        "    .resize((512, 512))\n",
        "    .convert(\"RGB\")\n",
        ")\n",
        "mask = (\n",
        "    load_image(\n",
        "        \"https://raw.githubusercontent.com/huggingface/amused/main/assets/inpainting_256_mask.png\"\n",
        "    )\n",
        "    .resize((512, 512))\n",
        "    .convert(\"L\")\n",
        ")    \n",
        "\n",
        "image = pipe(prompt, input_image, mask, generator=torch.manual_seed(3)).images[0]\n",
        "image.save(f\"inpainting_512.png\")\n",
        "```\n",
        "📜 Docs: https://huggingface.co/docs/diffusers/main/en/api/pipelines/amused\n",
        "\n",
        "🛠️ Models:\n",
        "\n",
        "- amused-256: https://huggingface.co/amused/amused-256 (603M params)\n",
        "- amused-512: https://huggingface.co/amused/amused-512 (608M params)\n",
        "\n"
      ],
      "metadata": {
        "id": "LoutoEHCNtfi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install diffusers\n",
        "!pip install accelerate"
      ],
      "metadata": {
        "id": "zvbZy1bLwuKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# 3 倍高速な SDXL\n",
        "\n",
        "TextToImage の拡散モデルの推論レイテンシーを高速化するために使用できる一連の最適化手法を紹介します。\n",
        "\n",
        "これらはすべて、追加の C++ コードを必要とせずにネイティブ PyTorch で実行できます。\n",
        "\n",
        "![](https://github.com/huggingface/diffusers/assets/22957388/36d6cf13-5794-4332-b197-b07c279935a1)\n",
        "\n",
        "これらのテクニックは Stable Diffusion XL (SDXL) に固有のものではなく、他のテキストから画像への拡散モデルを改善するためにも使用できます。 以下に提供される詳細なドキュメントを確認することをお勧めします。\n",
        "\n",
        "📜 Docs: https://huggingface.co/docs/diffusers/main/en/tutorials/fast_diffusion\n"
      ],
      "metadata": {
        "id": "GRsSC0_wObOP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 中断可能なパイプライン\n",
        "\n",
        "Diffusionプロセスの中断は、ユーザーが中間結果に満足できない場合に生成プロセスを停止できるため、ディフューザーで動作する UI を構築する場合に特に便利です。 コールバックを使用してこれをパイプラインに組み込むことができます。\n",
        "\n",
        "このコールバック関数は、pipe、i、t、および callback_kwargs の引数を取る必要があります (これは返される必要があります)。 特定のステップ数の後に拡散プロセスを停止するには、パイプラインの _interrupt 属性を True に設定します。 コールバック内に独自のカスタム停止ロジックを自由に実装することもできます。\n",
        "\n",
        "この例では、num_inference_steps が 50 に設定されているにもかかわらず、拡散プロセスは 10 ステップ後に停止します。\n",
        "\n",
        "```\n",
        "from diffusers import StableDiffusionPipeline\n",
        "\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\")\n",
        "pipe.enable_model_cpu_offload()\n",
        "num_inference_steps = 50\n",
        "\n",
        "def interrupt_callback(pipe, i, t, callback_kwargs):\n",
        "    stop_idx = 10\n",
        "    if i == stop_idx:\n",
        "        pipe._interrupt = True\n",
        "\n",
        "    return callback_kwargs\n",
        "\n",
        "pipe(\n",
        "    \"A photo of a cat\",\n",
        "    num_inference_steps=num_inference_steps,\n",
        "    callback_on_step_end=interrupt_callback,\n",
        ")\n",
        "```\n",
        "📜 ドキュメント: https://huggingface.co/docs/diffusers/main/en/using-diffusers/callback\n",
        "\n",
        "LoRA トレーニングの例におけるペフト\n",
        "LoRA に関して公式にサポートされているすべてのトレーニング例に peft を組み込みました。 これによりコードが大幅に簡素化され、可読性が向上します。 peft のおかげで、LoRA のトレーニングがさらに簡単になりました。\n",
        "\n",
        "## LCM LoRA SDXL トレーニングのよりメモリに優しいバージョン\n",
        "\n",
        "peft のベスト プラクティスを取り入れて、SDXL の LCM LoRA トレーニングをよりメモリに優しいものにしました。 そのため、2 つの UNet (教師と生徒) を初期化する必要はなくなりました。 このバージョンには、迅速な実験のためにデータセット ライブラリも統合されています。 詳細については、[このセクション](https://github.com/huggingface/diffusers/blob/main/examples/consistency_distillation/README_sdxl.md)をご覧ください。"
      ],
      "metadata": {
        "id": "sm_rcCzONHHX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from diffusers import AmusedPipeline\n",
        "\n",
        "pipe = AmusedPipeline.from_pretrained(\n",
        "    \"amused/amused-512\", variant=\"fp16\", torch_dtype=torch.float16\n",
        ")\n",
        "pipe.vqvae.to(torch.float32)  # vqvae is producing nans in fp16\n",
        "pipe = pipe.to(\"cuda\")\n",
        "\n",
        "prompt = \"miku\"\n",
        "image = pipe(prompt, generator=torch.manual_seed(39)).images[0]\n",
        "image.save(\"miku.png\")"
      ],
      "metadata": {
        "id": "LQO4SwPJ3kVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from diffusers import AmusedPipeline\n",
        "\n",
        "# Initialize the pipeline\n",
        "pipe = AmusedPipeline.from_pretrained(\n",
        "    \"amused/amused-512\", variant=\"fp16\", torch_dtype=torch.float16\n",
        ")\n",
        "pipe.vqvae.to(torch.float32)  # Fix for vqvae producing NaNs in fp16\n",
        "pipe = pipe.to(\"cuda\")\n",
        "\n",
        "# Define your prompt\n",
        "num_images = 9\n",
        "prompt = \"miku in dancing\"\n",
        "for i in range(num_images):\n",
        "    image = pipe(prompt, generator=torch.manual_seed(i)).images[0]\n",
        "    image.save(f\"miku_in_dancing{i}.png\")\n"
      ],
      "metadata": {
        "id": "oaqbfpYm2thv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TlRllIt6wWZg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from diffusers import AmusedImg2ImgPipeline\n",
        "from diffusers.utils import load_image\n",
        "\n",
        "pipe = AmusedImg2ImgPipeline.from_pretrained(\n",
        "    \"amused/amused-512\", variant=\"fp16\", torch_dtype=torch.float16\n",
        ")\n",
        "pipe.vqvae.to(torch.float32)  # vqvae is producing nans in fp16\n",
        "pipe = pipe.to(\"cuda\")\n",
        "\n",
        "prompt = \"apple watercolor\"\n",
        "input_image = (\n",
        "    load_image(\n",
        "        \"miku_39.png\"\n",
        "    )\n",
        "    .resize((512, 512))\n",
        "    .convert(\"RGB\")\n",
        ")\n",
        "\n",
        "image = pipe(prompt, input_image, strength=0.7, generator=torch.manual_seed(39)).images[0]\n",
        "image.save(\"image2image_512.png\")"
      ]
    }
  ]
}