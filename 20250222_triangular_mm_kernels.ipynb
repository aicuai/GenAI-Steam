{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOUdvhZtJaPuWfLio0ysRlw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aicuai/GenAI-Steam/blob/main/20250222_triangular_mm_kernels.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CUDA Triangular Matrix Multiplication Benchmark\n",
        "\n",
        "## Purpose\n",
        "\n",
        "This project aims to compare the performance of various CUDA kernel implementations for lower triangular matrix multiplication with PyTorch's built-in functions and to investigate a suspected \"memory stealing\" optimization technique.\n",
        "\n",
        "## Background\n",
        "\n",
        "A CUDA code (referred to as the \"Sakana\" version) published by a Japanese unicorn company was found to be unusually fast, but with incorrect calculation results.  It was suspected that this code might be \"stealing\" results from PyTorch's calculations by omitting memory initialization and CUDA kernel synchronization.\n",
        "\n",
        "This repository compares and validates the following CUDA kernel implementations and PyTorch's built-in functions:\n",
        "\n",
        "*   **Sakana:** The original, suspicious code (1D grid, no zero-initialization, no synchronization).\n",
        "*   **Improved:** A corrected version of the Sakana code, made to work correctly (2D grid, zero-initialization, synchronization).\n",
        "*   **Gemini:** A version attempting different optimizations (2D grid, zero-initialization, synchronization).\n",
        "*   **Gemini2:** Gemini version with shared memory and Cooperative Groups optimizations.\n",
        "*   **Gemini3:** A version modified to intentionally perform \"memory stealing\" (for comparative verification).\n",
        "*   **GeminiMMA:** Optimized using Warp Matrix Multiply-Accumulate (WMMA).\n",
        "*   **PyTorch:** Implementation using PyTorch's built-in functions (`torch.matmul` and `.tril()`).\n",
        "\n",
        "## Validation Methodology\n",
        "\n",
        "1.  **Correctness Validation (`allclose`):**\n",
        "    *   The `torch.allclose` function is used to check if the calculation results of each CUDA kernel and PyTorch match within a certain tolerance.\n",
        "    *   The execution order of the CUDA kernel and PyTorch is swapped to verify that the results do not change (\"memory stealing\" check).\n",
        "\n",
        "2.  **Performance Measurement (`do_bench`):**\n",
        "    *   The `triton.testing.do_bench` function is used to measure the execution time of each implementation.\n",
        "    *   The average execution time over multiple runs is compared.\n",
        "\n",
        "## Usage\n",
        "\n",
        "1.  **Environment:**\n",
        "    *   A GPU with CUDA support (NVIDIA Tesla T4 recommended)\n",
        "    *   Python 3.11\n",
        "    *   PyTorch (2.x or higher, CUDA-enabled version)\n",
        "    *   Required packages: `triton`, `ninja`, `setuptools`\n",
        "\n",
        "        ```bash\n",
        "        pip install --upgrade --force-reinstall torch torchvision torchaudio\n",
        "        pip install triton ninja setuptools\n",
        "        ```\n",
        "    *   (Note) In a Colab environment, errors may occur during the installation of `torch`. If this happens, try restarting the runtime and installing `torch` without specifying `--index-url`.\n",
        "\n",
        "2.  **Execution:**\n",
        "    *   Copy and paste the Python code from this repository into a Google Colab code cell.\n",
        "    *   Run the code cell.\n",
        "\n",
        "## Results\n",
        "\n",
        "| Version        | CUDA first | Torch first | allclose | Execution Time (ms) | Notes                                      |\n",
        "| :------------- | :--------- | :---------- | :------- | -----------------: | :----------------------------------------- |\n",
        "| Improved       | True       | True        | True     |     (measurement)     | Works correctly                            |\n",
        "| Gemini         | True       | True        | True     |       (measurement)    | Works correctly                            |\n",
        "| Gemini2        |      |      |         |     (measurement)      | Bug, or optimization unsuitable for GPU/problem |\n",
        "| Gemini3        | False      | False       | False    | (measurement)  | Intentional cheating (memory stealing)   |\n",
        "| GeminiMMA      |            |            |          | (measurement)       | Optimized using WMMA                        |\n",
        "| Sakana         | False      | True        | -       | (measurement)      | Unfair speedup due to \"memory stealing\"    |\n",
        "| PyTorch        | -           | -        |  -   |   (measurement)        |                                            |\n",
        "\n",
        "*   **Sakana:** `allclose` is `True` only when PyTorch is executed first, confirming \"memory stealing.\"\n",
        "*   **Gemini3:** Intentionally incorrect code, `allclose` is always `False`.\n",
        "* **Gemini2:** Implemented optimization using shared memory and cooperative groups\n",
        "* **GeminiMMA**: Implemented optimization with WMMA. Results and performance can significantly change depending on the environment.\n",
        "*   **Improved, Gemini:** Work correctly, and `allclose` is always `True`.\n",
        "*   **PyTorch:** In many cases, PyTorch's built-in functions were faster than manually optimized CUDA kernels. This is likely because PyTorch uses highly optimized libraries like cuBLAS internally.\n",
        "\n",
        "## Execution Environment\n",
        "\n",
        "*   Python: (version)\n",
        "*   OS: (version)\n",
        "*   CUDA: (version)\n",
        "*   PyTorch: (version)\n",
        "*   cuDNN: (version)\n",
        "*   GPU: (model name, e.g., NVIDIA Tesla T4)\n",
        "*   (Output of `nvidia-smi`)\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "This investigation confirmed that the original Sakana CUDA code has serious issues and achieves unfair speedups through \"memory stealing.\"  It was also found that PyTorch's built-in functions are often faster than manually optimized CUDA kernels in many cases.\n",
        "\n",
        "CUDA kernel optimization depends heavily on the GPU architecture, the characteristics of the problem, and a deep understanding of CUDA programming.  Simply rewriting code does not always improve performance, and in some cases, it may be better to rely on highly optimized libraries like PyTorch."
      ],
      "metadata": {
        "id": "I064MzhI8RJT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CUDA Triangular Matrix Multiplication Benchmark\n",
        "\n",
        "## 目的\n",
        "\n",
        "このプロジェクトは、下三角行列乗算 (Triangular Matrix Multiplication) のための വിവിധなCUDAカーネル実装とPyTorchの組み込み関数とのパフォーマンス比較、および「盗み見」と呼ばれる不正な高速化手法の検証を目的としています。\n",
        "\n",
        "## 背景\n",
        "\n",
        "ある日本のユニコーン企業が公開したCUDAコード (Sakanaバージョンと呼ぶ) が、異常に高速であるにもかかわらず、計算結果が不正であるという疑惑が持ち上がりました。このコードは、メモリの初期化を省略し、CUDAカーネルの同期を行わないことで、PyTorchの計算結果を「盗み見」している可能性が指摘されました。\n",
        "\n",
        "このリポジトリでは、以下のCUDAカーネル実装とPyTorchの組み込み関数を比較検証します。\n",
        "\n",
        "*   **Sakana:** オリジナルの疑惑のコード (1次元グリッド、ゼロ初期化なし、同期なし)。\n",
        "*   **Improved:** Sakanaバージョンを修正し、正しく動作するようにしたバージョン (2次元グリッド、ゼロ初期化、同期あり)。\n",
        "*   **Gemini:** 別の最適化を試みたバージョン (2次元グリッド、ゼロ初期化、同期あり)。\n",
        "*   **Gemini2:** Geminiバージョンに共有メモリとCooperative Groupsを使った最適化を追加したバージョン。\n",
        "*   **Gemini3:** 意図的に「盗み見」を行うように改変したバージョン (比較検証用)。\n",
        "*    **GeminiMMA:** Warp Matrix Multiply-Accumulate (WMMA) を使って最適化。\n",
        "*   **PyTorch:** PyTorchの組み込み関数 (`torch.matmul` と `.tril()`) を使用した実装。\n",
        "\n",
        "## 検証方法\n",
        "\n",
        "1.  **正当性検証 (`allclose`):**\n",
        "    *   各CUDAカーネルとPyTorchの計算結果が、ある許容誤差内で一致するかどうかを `torch.allclose` 関数で確認します。\n",
        "    *   CUDAカーネルの実行順序とPyTorchの実行順序を入れ替えて、結果が変わらないかを確認します (「盗み見」検証)。\n",
        "\n",
        "2.  **パフォーマンス測定 (`do_bench`):**\n",
        "    *   `triton.testing.do_bench` 関数を使用して、各実装の実行時間を計測します。\n",
        "    *   複数回の実行時間の平均値を比較します。\n",
        "\n",
        "## 使用方法\n",
        "\n",
        "1.  **環境:**\n",
        "    *   CUDAが利用可能なGPU (NVIDIA Tesla T4を推奨)\n",
        "    *   Python 3.11\n",
        "    *   PyTorch (2.x以上、CUDA対応版)\n",
        "    *   必要なパッケージ: `triton`, `ninja`, `setuptools`\n",
        "\n",
        "        ```bash\n",
        "        pip install --upgrade --force-reinstall torch torchvision torchaudio\n",
        "        pip install triton ninja setuptools\n",
        "        ```\n",
        "    *   (注意) Colab環境では、`torch`のインストールでエラーが発生することがあります。その場合は、ランタイムを再起動し、`--index-url` を指定せずに `pip install torch`を実行する、などの対応が必要です。\n",
        "\n",
        "2.  **実行:**\n",
        "    *   このリポジトリのPythonコードを、Google Colabのコードセルにコピー＆ペーストします。\n",
        "    *   コードセルを実行します。\n",
        "\n",
        "## 結果\n",
        "\n",
        "| バージョン     | CUDA first | Torch first | allclose | 実行時間 (ms) | 備考                               |\n",
        "| :------------- | :--------- | :---------- | :------- | -------------: | :--------------------------------- |\n",
        "| Improved       | True       | True        | True     |     (計測結果)     | 正しく動作                         |\n",
        "| Gemini         | True       | True        | True     |       (計測結果)        | 正しく動作                         |\n",
        "| Gemini2        |      |      |         |     (計測結果)          |       バグまたは、最適化がGPU/問題設定に合わない                             |\n",
        "| Gemini3      | False      | False       | False   | (計測結果)      | 意図的な不正 (盗み見)               |\n",
        "| GeminiMMA     |       |      |         |       (計測結果)        |       WMMAによる最適化                             |\n",
        "| Sakana         | False      | True        | -       | (計測結果)      | 「盗み見」による不正な高速化       |\n",
        "| PyTorch        | -           | -        |  -   |   (計測結果)   |                                    |\n",
        "\n",
        "*   **Sakana:** PyTorchを先に実行した場合にのみ `allclose` が `True` になり、「盗み見」が確認されました。\n",
        "*   **Gemini3:** 意図的に不正なコードにしたもので、常に`allclose` が `False`。\n",
        "*   **Gemini2:**  共有メモリとCooperative Groupsを使った最適化を実装。\n",
        "*    **GeminiMMA:** WMMAを使った最適化を実装。 結果とパフォーマンスは実行環境によって大きく変わる可能性があります。\n",
        "*   **Improved, Gemini:** 正しく動作し、`allclose` は常に `True` です。\n",
        "*   **PyTorch:** 多くのケースで、PyTorchの組み込み関数が、手動で最適化したCUDAカーネルよりも高速でした。これは、PyTorchが内部でcuBLASなどの高度に最適化されたライブラリを使用しているためと考えられます。\n",
        "\n",
        "## 実行環境\n",
        "\n",
        "*   Python: (バージョン)\n",
        "*   OS: (バージョン)\n",
        "*   CUDA: (バージョン)\n",
        "*   PyTorch: (バージョン)\n",
        "*   cuDNN: (バージョン)\n",
        "*   GPU: (モデル名, 例: NVIDIA Tesla T4)\n",
        "*   (nvidia-smi の出力)\n",
        "\n",
        "## 結論\n",
        "\n",
        "今回の検証により、オリジナルのSakana CUDAコードには重大な問題があり、「盗み見」によって不正に高速化されていることが確認されました。 また、PyTorchの組み込み関数は、多くの場合、手動で最適化したCUDAカーネルよりも高速であることもわかりました。\n",
        "\n",
        "CUDAカーネルの最適化は、GPUアーキテクチャ、問題の特性、そしてCUDAプログラミングの深い知識に依存します。 単純なコードの書き換えだけでは、必ずしもパフォーマンスが向上するとは限らず、場合によっては、PyTorchのような高度に最適化されたライブラリに任せる方が良い結果が得られることもあります。\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "cCnAulVu7g2p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --force-reinstall torch torchvision torchaudio\n",
        "!pip install triton ninja"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "w5Pt2pu6wSRQ",
        "outputId": "cf03b083-a8f4-47c6-f812-9ad63b857b21"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch\n",
            "  Downloading torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "Collecting torchvision\n",
            "  Downloading torchvision-0.21.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
            "Collecting torchaudio\n",
            "  Downloading torchaudio-2.6.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting filelock (from torch)\n",
            "  Downloading filelock-3.17.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting typing-extensions>=4.10.0 (from torch)\n",
            "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting networkx (from torch)\n",
            "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting jinja2 (from torch)\n",
            "  Downloading jinja2-3.1.5-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting fsspec (from torch)\n",
            "  Downloading fsspec-2025.2.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparselt-cu12==0.6.2 (from torch)\n",
            "  Downloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting nvidia-nccl-cu12==2.21.5 (from torch)\n",
            "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting triton==3.2.0 (from torch)\n",
            "  Downloading triton-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Collecting sympy==1.13.1 (from torch)\n",
            "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch)\n",
            "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting numpy (from torchvision)\n",
            "  Downloading numpy-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m852.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pillow!=8.3.*,>=5.3.0 (from torchvision)\n",
            "  Downloading pillow-11.1.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (9.1 kB)\n",
            "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
            "  Downloading MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
            "Downloading torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl (766.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m766.7/766.7 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m101.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.2/253.2 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.21.0-cp311-cp311-manylinux1_x86_64.whl (7.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m90.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchaudio-2.6.0-cp311-cp311-manylinux1_x86_64.whl (3.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m84.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow-11.1.0-cp311-cp311-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m69.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
            "Downloading filelock-3.17.0-py3-none-any.whl (16 kB)\n",
            "Downloading fsspec-2025.2.0-py3-none-any.whl (184 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.5/184.5 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jinja2-3.1.5-py3-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.6/134.6 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m74.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m83.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\n",
            "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, nvidia-cusparselt-cu12, mpmath, typing-extensions, sympy, pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, MarkupSafe, fsspec, filelock, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jinja2, nvidia-cusolver-cu12, torch, torchvision, torchaudio\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.1.0\n",
            "    Uninstalling triton-3.1.0:\n",
            "      Successfully uninstalled triton-3.1.0\n",
            "  Attempting uninstall: mpmath\n",
            "    Found existing installation: mpmath 1.3.0\n",
            "    Uninstalling mpmath-1.3.0:\n",
            "      Successfully uninstalled mpmath-1.3.0\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.12.2\n",
            "    Uninstalling typing_extensions-4.12.2:\n",
            "      Successfully uninstalled typing_extensions-4.12.2\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.13.1\n",
            "    Uninstalling sympy-1.13.1:\n",
            "      Successfully uninstalled sympy-1.13.1\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: pillow 11.1.0\n",
            "    Uninstalling pillow-11.1.0:\n",
            "      Successfully uninstalled pillow-11.1.0\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.4.127\n",
            "    Uninstalling nvidia-nvtx-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.21.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.21.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "  Attempting uninstall: networkx\n",
            "    Found existing installation: networkx 3.4.2\n",
            "    Uninstalling networkx-3.4.2:\n",
            "      Successfully uninstalled networkx-3.4.2\n",
            "  Attempting uninstall: MarkupSafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "  Attempting uninstall: filelock\n",
            "    Found existing installation: filelock 3.17.0\n",
            "    Uninstalling filelock-3.17.0:\n",
            "      Successfully uninstalled filelock-3.17.0\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: jinja2\n",
            "    Found existing installation: Jinja2 3.1.5\n",
            "    Uninstalling Jinja2-3.1.5:\n",
            "      Successfully uninstalled Jinja2-3.1.5\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.5.1+cu124\n",
            "    Uninstalling torch-2.5.1+cu124:\n",
            "      Successfully uninstalled torch-2.5.1+cu124\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.20.1+cu124\n",
            "    Uninstalling torchvision-0.20.1+cu124:\n",
            "      Successfully uninstalled torchvision-0.20.1+cu124\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.5.1+cu124\n",
            "    Uninstalling torchaudio-2.5.1+cu124:\n",
            "      Successfully uninstalled torchaudio-2.5.1+cu124\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.3 which is incompatible.\n",
            "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.2.3 which is incompatible.\n",
            "thinc 8.2.5 requires numpy<2.0.0,>=1.19.0; python_version >= \"3.9\", but you have numpy 2.2.3 which is incompatible.\n",
            "fastai 2.7.18 requires torch<2.6,>=1.10, but you have torch 2.6.0 which is incompatible.\n",
            "numba 0.61.0 requires numpy<2.2,>=1.24, but you have numpy 2.2.3 which is incompatible.\n",
            "pytensor 2.27.1 requires numpy<2,>=1.17.0, but you have numpy 2.2.3 which is incompatible.\n",
            "langchain 0.3.19 requires numpy<2,>=1.26.4; python_version < \"3.12\", but you have numpy 2.2.3 which is incompatible.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2025.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed MarkupSafe-3.0.2 filelock-3.17.0 fsspec-2025.2.0 jinja2-3.1.5 mpmath-1.3.0 networkx-3.4.2 numpy-2.2.3 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-cusparselt-cu12-0.6.2 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 pillow-11.1.0 sympy-1.13.1 torch-2.6.0 torchaudio-2.6.0 torchvision-0.21.0 triton-3.2.0 typing-extensions-4.12.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              },
              "id": "ffdf41d5302f463980c1435d2d8f22db"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: triton in /usr/local/lib/python3.11/dist-packages (3.2.0)\n",
            "Collecting ninja\n",
            "  Downloading ninja-1.11.1.3-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.3 kB)\n",
            "Downloading ninja-1.11.1.3-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.9/422.9 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ninja\n",
            "Successfully installed ninja-1.11.1.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y79ruPO1v9Bk",
        "outputId": "bb52beb5-830a-47cd-f0ea-ccd7d3c549fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "検証 (Improved):\n",
            "Improved: allclose = True\n",
            "\n",
            "検証 (Gemini):\n",
            "Gemini: allclose = True\n",
            "\n",
            "検証 (Sakana):\n",
            "Sakana: allclose = False\n",
            "Max absolute difference: tensor(288.5991, device='cuda:0')\n",
            "\n",
            "ベンチマーク:\n",
            "Improved (CUDA):\n",
            "Gemini (CUDA):\n",
            "Sakana (CUDA):\n",
            "PyTorch:\n",
            "\n",
            "--- 結果 ---\n",
            "Improved (CUDA) の実行時間: 63.6253 ms\n",
            "Gemini (CUDA)   の実行時間: 77.1644 ms\n",
            "Sakana (CUDA)   の実行時間: 0.2688 ms\n",
            "PyTorch の実行時間: 29.0856 ms\n",
            "\n",
            "CUDA内比較: Sakana版が高速 (速度向上率: 287.07倍)\n",
            "全体比較: Sakana (CUDA) が高速 (速度向上率: 108.20倍)\n",
            "\n",
            "--- 実行環境 ---\n",
            "  Python: 3.11.11\n",
            "  OS: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "  CUDA: 12.4\n",
            "  PyTorch: 2.6.0+cu124\n",
            "  cuDNN: 90100\n",
            "Fri Feb 21 18:40:25 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   69C    P0             68W /   70W |     918MiB /  15360MiB |    100%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.cpp_extension import load\n",
        "from triton.testing import do_bench\n",
        "import random\n",
        "import platform\n",
        "import subprocess\n",
        "\n",
        "# --- CUDA C++ コード ---\n",
        "\n",
        "# 1. Improved (2Dグリッド、ゼロ初期化、同期)\n",
        "cu_code_improved = \"\"\"\n",
        "#include <torch/extension.h>\n",
        "#include <cuda.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "__global__ void triangular_mm_kernel(const float* __restrict__ A,\n",
        "                                      const float* __restrict__ B,\n",
        "                                      float* __restrict__ C, const int N) {\n",
        "  const int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "  const int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "  if (row < N && col < N) {\n",
        "    if (col <= row) {\n",
        "      float sum = 0.0f;\n",
        "      #pragma unroll 8\n",
        "      for (int k = col; k <= row; k++) {\n",
        "        sum += A[row * N + k] * B[k * N + col];\n",
        "      }\n",
        "      C[row * N + col] = sum;\n",
        "    } else {\n",
        "      C[row * N + col] = 0.0f;\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "at::Tensor forward(at::Tensor A, at::Tensor B) {\n",
        "  TORCH_CHECK(A.is_cuda(), \"A must be a CUDA tensor\");\n",
        "  TORCH_CHECK(B.is_cuda(), \"B must be a CUDA tensor\");\n",
        "  TORCH_CHECK(A.dim() == 2, \"A must be a 2D tensor\");\n",
        "  TORCH_CHECK(B.dim() == 2, \"B must be a 2D tensor\");\n",
        "  TORCH_CHECK(A.size(0) == A.size(1), \"A must be square\");\n",
        "  TORCH_CHECK(B.size(0) == B.size(1), \"B must be square\");\n",
        "  TORCH_CHECK(A.size(0) == B.size(0), \"A and B must be the same size\");\n",
        "\n",
        "  int N = A.size(0);\n",
        "  auto C = torch::zeros_like(A); // ゼロで初期化\n",
        "\n",
        "  const int TILE_WIDTH = 32;\n",
        "  dim3 blockDim(TILE_WIDTH, TILE_WIDTH);\n",
        "  dim3 gridDim((N + TILE_WIDTH - 1) / TILE_WIDTH, (N + TILE_WIDTH - 1) / TILE_WIDTH);\n",
        "\n",
        "  triangular_mm_kernel<<<gridDim, blockDim>>>(\n",
        "      A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n",
        "\n",
        "  cudaDeviceSynchronize(); // CUDAカーネルの完了を待つ\n",
        "  cudaError_t err = cudaGetLastError();\n",
        "  TORCH_CHECK(err == cudaSuccess, \"CUDA kernel failed: \", cudaGetErrorString(err));\n",
        "  return C;\n",
        "}\n",
        "\n",
        "PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n",
        "  m.def(\"forward\", &forward, \"Improved CUDA\");\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# 2. Gemini (2D, ゼロ初期化、同期)\n",
        "cu_code_gemini = \"\"\"\n",
        "#include <torch/extension.h>\n",
        "#include <cuda.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "__global__ void triangular_mm_kernel_gemini(const float* __restrict__ A,\n",
        "                                            const float* __restrict__ B,\n",
        "                                            float* __restrict__ C, const int N) {\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if(row < N && col < N) {\n",
        "        if (col <= row) {\n",
        "            float sum = 0.0f;\n",
        "            #pragma unroll\n",
        "            for (int k = col; k <= row; ++k) {\n",
        "                sum += A[row * N + k] * B[k * N + col];\n",
        "            }\n",
        "             C[row * N + col] = sum;\n",
        "        }\n",
        "        else{\n",
        "            C[row * N + col] = 0.0f;\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "at::Tensor forward_gemini(at::Tensor A, at::Tensor B) {\n",
        "    TORCH_CHECK(A.is_cuda(), \"A must be a CUDA tensor\");\n",
        "    TORCH_CHECK(B.is_cuda(), \"B must be a CUDA tensor\");\n",
        "    TORCH_CHECK(A.dim() == 2, \"A must be a 2D tensor\");\n",
        "    TORCH_CHECK(B.dim() == 2, \"B must be a 2D tensor\");\n",
        "    TORCH_CHECK(A.size(0) == A.size(1), \"A must be square.\");\n",
        "    TORCH_CHECK(B.size(0) == B.size(1), \"B must be square.\");\n",
        "    TORCH_CHECK(A.size(0) == B.size(0), \"A and B must have the same size.\");\n",
        "\n",
        "    int N = A.size(0);\n",
        "    auto C = torch::zeros_like(A);\n",
        "\n",
        "    const int TILE_SIZE = 32;\n",
        "    dim3 threads(TILE_SIZE, TILE_SIZE);\n",
        "    dim3 blocks((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n",
        "\n",
        "    triangular_mm_kernel_gemini<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n",
        "\n",
        "    cudaDeviceSynchronize(); // CUDAカーネルの完了を待つ\n",
        "    cudaError_t err = cudaGetLastError();\n",
        "    if (err != cudaSuccess) {\n",
        "        printf(\"CUDA error: %s\\\\n\", cudaGetErrorString(err));\n",
        "    }\n",
        "    return C;\n",
        "}\n",
        "\n",
        "PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n",
        "  m.def(\"forward_gemini\", &forward_gemini, \"Gemini CUDA\");\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# 3. Sakana (オリジナル、1Dグリッド、ゼロ初期化なし、同期なし)\n",
        "cu_code_sakana = \"\"\"\n",
        "#include <torch/extension.h>\n",
        "#include <cuda.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "__global__ void triangular_mm_kernel(const float* __restrict__ A,\n",
        "                                      const float* __restrict__ B,\n",
        "                                      float* __restrict__ C, const int N) {\n",
        "  const int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "  const int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "  if (row < N && col < N) {\n",
        "    if (col <= row) {\n",
        "      float sum = 0.0f;\n",
        "      #pragma unroll 8\n",
        "      for (int k = col; k <= row; k++) {\n",
        "        sum += A[row * N + k] * B[k * N + col];\n",
        "      }\n",
        "      C[row * N + col] = sum;\n",
        "    } else {\n",
        "      C[row * N + col] = 0.0f;\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "at::Tensor forward(at::Tensor A, at::Tensor B) {\n",
        "  TORCH_CHECK(A.is_cuda(), \"A must be a CUDA tensor\");\n",
        "  TORCH_CHECK(B.is_cuda(), \"B must be a CUDA tensor\");\n",
        "  TORCH_CHECK(A.dim() == 2, \"A must be a 2D tensor\");\n",
        "  TORCH_CHECK(B.dim() == 2, \"B must be a 2D tensor\");\n",
        "  TORCH_CHECK(A.size(0) == A.size(1), \"A must be square\");\n",
        "  TORCH_CHECK(B.size(0) == B.size(1), \"B must be square\");\n",
        "  TORCH_CHECK(A.size(0) == B.size(0), \"A and B must be the same size\");\n",
        "\n",
        "  int N = A.size(0);\n",
        "  auto C = torch::empty_like(A); // ゼロ初期化しない\n",
        "\n",
        "  const int threadsPerBlock = 256;\n",
        "  const int numBlocks = N;\n",
        "\n",
        "  triangular_mm_kernel<<<numBlocks, threadsPerBlock>>>(\n",
        "      A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n",
        "\n",
        "  // cudaDeviceSynchronize();  // 同期しない\n",
        "  cudaError_t err = cudaGetLastError();\n",
        "  TORCH_CHECK(err == cudaSuccess, \"CUDA kernel failed: \", cudaGetErrorString(err));\n",
        "  return C;\n",
        "}\n",
        "\n",
        "PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n",
        "  m.def(\"forward\", &forward, \"Sakana CUDA\");\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# --- Python コード ---\n",
        "\n",
        "# CUDAコードをファイルに書き出す\n",
        "with open(\"tmp_improved.cu\", \"w\") as f:\n",
        "    f.write(cu_code_improved)\n",
        "with open(\"tmp_gemini.cu\", \"w\") as f:\n",
        "    f.write(cu_code_gemini)\n",
        "with open(\"tmp_sakana.cu\", \"w\") as f:\n",
        "    f.write(cu_code_sakana)\n",
        "\n",
        "# CUDA拡張をロード\n",
        "cuda_fn_improved = load(\n",
        "    name=\"triangular_mm_improved\",\n",
        "    sources=[\"tmp_improved.cu\"],\n",
        "    extra_cuda_cflags=[\"-O3\", \"--use_fast_math\"],\n",
        "    with_cuda=True,\n",
        "    verbose=False,\n",
        ").forward\n",
        "\n",
        "try:\n",
        "    cuda_fn_gemini = load(\n",
        "        name=\"triangular_mm_gemini\",\n",
        "        sources=[\"tmp_gemini.cu\"],\n",
        "        extra_cuda_cflags=[\"-O3\", \"--use_fast_math\"],\n",
        "        with_cuda=True,\n",
        "        verbose=False,\n",
        "    ).forward_gemini\n",
        "except Exception as e:\n",
        "    print(f\"Gemini版のロードに失敗しました: {e}\")\n",
        "    cuda_fn_gemini = None\n",
        "\n",
        "cuda_fn_sakana = load(\n",
        "    name=\"triangular_mm_sakana\",\n",
        "    sources=[\"tmp_sakana.cu\"],\n",
        "    extra_cuda_cflags=[\"-O3\", \"--use_fast_math\"],\n",
        "    with_cuda=True,\n",
        "    verbose=False,\n",
        ").forward\n",
        "\n",
        "N = 4096\n",
        "\n",
        "# PyTorchの比較用関数\n",
        "def trilmm(a, b):\n",
        "    return torch.matmul(a, b).tril()\n",
        "\n",
        "# 検証関数 (CUDAを先に実行)\n",
        "def validate(fn, name):\n",
        "    random.seed(42)\n",
        "    torch.manual_seed(42)\n",
        "    a = torch.tril(torch.randn(N, N, device=\"cuda\"))\n",
        "    b = torch.tril(torch.randn(N, N, device=\"cuda\"))\n",
        "    c_cuda = fn(a, b)\n",
        "    c_torch = trilmm(a, b)\n",
        "    is_close = torch.allclose(c_cuda, c_torch)\n",
        "    print(f\"{name}: allclose = {is_close}\")\n",
        "    if not is_close:\n",
        "        print(\"Max absolute difference:\", (c_cuda - c_torch).abs().max())\n",
        "\n",
        "# 検証を実行\n",
        "print(\"検証 (Improved):\")\n",
        "validate(cuda_fn_improved, \"Improved\")\n",
        "if cuda_fn_gemini:\n",
        "    print(\"\\n検証 (Gemini):\")\n",
        "    validate(cuda_fn_gemini, \"Gemini\")\n",
        "print(\"\\n検証 (Sakana):\")\n",
        "validate(cuda_fn_sakana, \"Sakana\") # Sakana版の検証\n",
        "\n",
        "\n",
        "# ベンチマーク (GPUが利用可能で、関数がロードされている場合のみ)\n",
        "if torch.cuda.is_available():\n",
        "    a = torch.tril(torch.randn(N, N, device=\"cuda\"))\n",
        "    b = torch.tril(torch.randn(N, N, device=\"cuda\"))\n",
        "\n",
        "    print(\"\\nベンチマーク:\")\n",
        "    print(\"Improved (CUDA):\")\n",
        "    improved_time = do_bench(lambda: cuda_fn_improved(a, b).mean())\n",
        "\n",
        "    if cuda_fn_gemini:\n",
        "        print(\"Gemini (CUDA):\")\n",
        "        gemini_time = do_bench(lambda: cuda_fn_gemini(a, b).mean())\n",
        "\n",
        "    print(\"Sakana (CUDA):\")\n",
        "    sakana_time = do_bench(lambda: cuda_fn_sakana(a, b).mean())\n",
        "\n",
        "    print(\"PyTorch:\")\n",
        "    pytorch_time = do_bench(lambda: trilmm(a, b).mean())\n",
        "\n",
        "    # 結果の比較と表示\n",
        "    print(\"\\n--- 結果 ---\")\n",
        "    print(f\"Improved (CUDA) の実行時間: {improved_time:.4f} ms\")\n",
        "    if cuda_fn_gemini:\n",
        "        print(f\"Gemini (CUDA)   の実行時間: {gemini_time:.4f} ms\")\n",
        "    print(f\"Sakana (CUDA)   の実行時間: {sakana_time:.4f} ms\")\n",
        "    print(f\"PyTorch の実行時間: {pytorch_time:.4f} ms\")\n",
        "\n",
        "    # CUDA内比較\n",
        "    if cuda_fn_gemini:\n",
        "        fastest_cuda = \"Improved\"\n",
        "        if gemini_time < improved_time:\n",
        "            fastest_cuda = \"Gemini\"\n",
        "        if sakana_time < min(improved_time, gemini_time):\n",
        "            fastest_cuda = \"Sakana\"\n",
        "\n",
        "        if fastest_cuda == \"Improved\":\n",
        "          speedup_cuda = max(gemini_time, sakana_time) / improved_time\n",
        "        elif fastest_cuda == \"Gemini\":\n",
        "          speedup_cuda = max(improved_time, sakana_time) / gemini_time\n",
        "        else: # fastest_cuda == \"Sakana\":\n",
        "          speedup_cuda = max(improved_time, gemini_time) / sakana_time\n",
        "        print(f\"\\nCUDA内比較: {fastest_cuda}版が高速 (速度向上率: {speedup_cuda:.2f}倍)\")\n",
        "\n",
        "\n",
        "    # 全体比較\n",
        "    fastest_overall = \"PyTorch\"\n",
        "    if improved_time < pytorch_time:\n",
        "        fastest_overall = \"Improved (CUDA)\"\n",
        "    if cuda_fn_gemini and gemini_time < min(pytorch_time, improved_time):\n",
        "        fastest_overall = \"Gemini (CUDA)\"\n",
        "    if sakana_time < min(pytorch_time, improved_time, gemini_time if cuda_fn_gemini else float('inf')):\n",
        "        fastest_overall = \"Sakana (CUDA)\"\n",
        "\n",
        "    if fastest_overall == \"PyTorch\":\n",
        "        speedup = min(improved_time, gemini_time if cuda_fn_gemini else float('inf'), sakana_time) / pytorch_time\n",
        "    else:\n",
        "        speedup = pytorch_time / min(improved_time, gemini_time if cuda_fn_gemini else float('inf'), sakana_time)\n",
        "\n",
        "    print(f\"全体比較: {fastest_overall} が高速 (速度向上率: {speedup:.2f}倍)\")\n",
        "\n",
        "\n",
        "else:\n",
        "    print(\"CUDAが利用できないため、ベンチマークはスキップします。\")\n",
        "\n",
        "# 環境情報の出力\n",
        "print(\"\\n--- 実行環境 ---\")\n",
        "print(f\"  Python: {platform.python_version()}\")\n",
        "print(f\"  OS: {platform.platform()}\")\n",
        "try:\n",
        "    print(f\"  CUDA: {torch.version.cuda}\")\n",
        "    print(f\"  PyTorch: {torch.__version__}\")\n",
        "    print(f\"  cuDNN: {torch.backends.cudnn.version()}\")\n",
        "    print(subprocess.check_output([\"nvidia-smi\"]).decode())\n",
        "except:\n",
        "    print(\"  CUDA/PyTorch 情報の取得に失敗しました。\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# コードと結果は保存しましたので、もう1段階進めましょう。\n",
        "\n",
        "- Geminiの現在のコードに最適化案を Gemini2 を並列する\n",
        "- Geminiによるこのバグを利用したチートを入れたGemini3を提案する\n",
        "- 盗み見を検証するために入れ替えケースの結果を追加実施して表示する\n",
        "\n",
        "\n",
        "Gemini2: Geminiの現在のコードをベースに、さらなる最適化を施したバージョン (Gemini2) を作成します。\n",
        "\n",
        "Gemini3: Geminiのコードをベースに、意図的に「盗み見」を行うバージョン (Gemini3) を作成します。\n",
        "\n",
        "実行順序入れ替え検証: 全てのバージョン (Improved, Gemini, Gemini2, Gemini3, Sakana, PyTorch) で、CUDAカーネルとPyTorchの実行順序を入れ替えて allclose の結果を確認します。\n",
        "\n",
        "ベンチマーク: 全てのバージョンでベンチマークを行い、実行時間を比較します。"
      ],
      "metadata": {
        "id": "WgrAEym31gdS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 変更点:\n",
        "\n",
        "### Gemini2:\n",
        "\n",
        "Cooperative Groups (cooperative_groups.h) を使用: スレッドブロック全体の同期をより効率的に行う。\n",
        "\n",
        "Shared Memory (共有メモリ) を使用: 各スレッドブロック内で、A と B の一部を共有メモリにロードし、そこから計算を行うことで、グローバルメモリへのアクセスを減らす。\n",
        "\n",
        "ループ分割: k に関するループを TILE_SIZE (ここでは32) ごとに分割し、共有メモリを効果的に利用。\n",
        "\n",
        "-gencode=arch=compute_75,code=sm_75 を追加: 推奨アーキテクチャを指定\n",
        "\n",
        "### Gemini3:\n",
        "\n",
        "auto C = torch::empty_like(A);: C をゼロ初期化しない。\n",
        "\n",
        "cudaDeviceSynchronize(); をコメントアウト: 同期しない。\n",
        "\n",
        "計算部分を大幅に省略。\n",
        "\n",
        "検証関数 (validate):\n",
        "\n",
        "cuda_first パラメータを追加: CUDAカーネルとPyTorchのどちらを先に実行するかを指定できるようにした。\n",
        "\n",
        "## ベンチマーク:\n",
        "\n",
        "各バージョンの実行時間を辞書 times に格納。\n",
        "\n",
        "CUDA内での最速バージョンと、全体での最速バージョンを判定し、速度向上率を計算。\n",
        "\n",
        "これで、4つのCUDAバージョン (Improved, Gemini, Gemini2, Gemini3, Sakana) と PyTorch の比較検証ができます。特に、Gemini3 (盗み見バージョン) と Sakana が、実行順序によって allclose の結果が変わるかどうかに注目してください。"
      ],
      "metadata": {
        "id": "cvXzT-Ow2EZo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.cpp_extension import load\n",
        "from triton.testing import do_bench\n",
        "import random\n",
        "import platform\n",
        "import subprocess\n",
        "\n",
        "# --- CUDA C++ コード ---\n",
        "\n",
        "# 1. Improved (2Dグリッド、ゼロ初期化、同期)\n",
        "cu_code_improved = \"\"\"\n",
        "#include <torch/extension.h>\n",
        "#include <cuda.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "__global__ void triangular_mm_kernel(const float* __restrict__ A,\n",
        "                                      const float* __restrict__ B,\n",
        "                                      float* __restrict__ C, const int N) {\n",
        "  const int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "  const int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "  if (row < N && col < N) {\n",
        "    if (col <= row) {\n",
        "      float sum = 0.0f;\n",
        "      #pragma unroll 8\n",
        "      for (int k = col; k <= row; k++) {\n",
        "        sum += A[row * N + k] * B[k * N + col];\n",
        "      }\n",
        "      C[row * N + col] = sum;\n",
        "    } else {\n",
        "      C[row * N + col] = 0.0f;\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "at::Tensor forward(at::Tensor A, at::Tensor B) {\n",
        "    // ... (省略: 以前のバージョンと同じ) ...\n",
        "  TORCH_CHECK(A.is_cuda(), \"A must be a CUDA tensor\");\n",
        "  TORCH_CHECK(B.is_cuda(), \"B must be a CUDA tensor\");\n",
        "  TORCH_CHECK(A.dim() == 2, \"A must be a 2D tensor\");\n",
        "  TORCH_CHECK(B.dim() == 2, \"B must be a 2D tensor\");\n",
        "  TORCH_CHECK(A.size(0) == A.size(1), \"A must be square\");\n",
        "  TORCH_CHECK(B.size(0) == B.size(1), \"B must be square\");\n",
        "  TORCH_CHECK(A.size(0) == B.size(0), \"A and B must be the same size\");\n",
        "\n",
        "  int N = A.size(0);\n",
        "  auto C = torch::zeros_like(A); // ゼロで初期化\n",
        "\n",
        "  const int TILE_WIDTH = 32;\n",
        "  dim3 blockDim(TILE_WIDTH, TILE_WIDTH);\n",
        "  dim3 gridDim((N + TILE_WIDTH - 1) / TILE_WIDTH, (N + TILE_WIDTH - 1) / TILE_WIDTH);\n",
        "\n",
        "  triangular_mm_kernel<<<gridDim, blockDim>>>(\n",
        "      A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n",
        "\n",
        "  cudaDeviceSynchronize(); // CUDAカーネルの完了を待つ\n",
        "  cudaError_t err = cudaGetLastError();\n",
        "  TORCH_CHECK(err == cudaSuccess, \"CUDA kernel failed: \", cudaGetErrorString(err));\n",
        "  return C;\n",
        "}\n",
        "\n",
        "PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n",
        "  m.def(\"forward\", &forward, \"Improved CUDA\");\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# 2. Gemini (2D, ゼロ初期化、同期)\n",
        "cu_code_gemini = \"\"\"\n",
        "#include <torch/extension.h>\n",
        "#include <cuda.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "__global__ void triangular_mm_kernel_gemini(const float* __restrict__ A,\n",
        "                                            const float* __restrict__ B,\n",
        "                                            float* __restrict__ C, const int N) {\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if(row < N && col < N) {\n",
        "        if (col <= row) {\n",
        "            float sum = 0.0f;\n",
        "            #pragma unroll\n",
        "            for (int k = col; k <= row; ++k) {\n",
        "                sum += A[row * N + k] * B[k * N + col];\n",
        "            }\n",
        "             C[row * N + col] = sum;\n",
        "        }\n",
        "        else{\n",
        "            C[row * N + col] = 0.0f;\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "at::Tensor forward_gemini(at::Tensor A, at::Tensor B) {\n",
        "    // ... (省略: 以前のバージョンと同じ)\n",
        "    TORCH_CHECK(A.is_cuda(), \"A must be a CUDA tensor\");\n",
        "    TORCH_CHECK(B.is_cuda(), \"B must be a CUDA tensor\");\n",
        "    TORCH_CHECK(A.dim() == 2, \"A must be a 2D tensor\");\n",
        "    TORCH_CHECK(B.dim() == 2, \"B must be a 2D tensor\");\n",
        "    TORCH_CHECK(A.size(0) == A.size(1), \"A must be square.\");\n",
        "    TORCH_CHECK(B.size(0) == B.size(1), \"B must be square.\");\n",
        "    TORCH_CHECK(A.size(0) == B.size(0), \"A and B must have the same size.\");\n",
        "\n",
        "    int N = A.size(0);\n",
        "    auto C = torch::zeros_like(A);\n",
        "\n",
        "    const int TILE_SIZE = 32;\n",
        "    dim3 threads(TILE_SIZE, TILE_SIZE);\n",
        "    dim3 blocks((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n",
        "\n",
        "    triangular_mm_kernel_gemini<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n",
        "\n",
        "    cudaDeviceSynchronize(); // CUDAカーネルの完了を待つ\n",
        "    cudaError_t err = cudaGetLastError();\n",
        "    if (err != cudaSuccess) {\n",
        "        printf(\"CUDA error: %s\\\\n\", cudaGetErrorString(err));\n",
        "    }\n",
        "    return C;\n",
        "}\n",
        "\n",
        "PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n",
        "  m.def(\"forward_gemini\", &forward_gemini, \"Gemini CUDA\");\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# 3. Gemini2 (最適化案)\n",
        "cu_code_gemini2 = \"\"\"\n",
        "#include <torch/extension.h>\n",
        "#include <cuda.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include <cooperative_groups.h> // Cooperative Groups を使う\n",
        "\n",
        "namespace cg = cooperative_groups;\n",
        "\n",
        "__global__ void triangular_mm_kernel_gemini2(const float* __restrict__ A,\n",
        "                                             const float* __restrict__ B,\n",
        "                                             float* __restrict__ C, const int N) {\n",
        "  // Cooperative Groups でスレッドブロック全体を表すグループを作成\n",
        "  cg::thread_block cta = cg::this_thread_block();\n",
        "  int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "  int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "  if (row < N && col < N) {\n",
        "    if (col <= row) {\n",
        "      float sum = 0.0f;\n",
        "\n",
        "      // Shared Memory (共有メモリ) を使った最適化\n",
        "      __shared__ float shared_A[32][32];\n",
        "      __shared__ float shared_B[32][32];\n",
        "\n",
        "      // ループを TILE_SIZE で分割\n",
        "      for (int k_start = col; k_start <= row; k_start += 32) {\n",
        "          int k_end = min(k_start + 32, row + 1);\n",
        "\n",
        "          // 共有メモリに A と B の一部をロード\n",
        "          if (k_start + threadIdx.x < k_end) {\n",
        "              shared_A[threadIdx.y][threadIdx.x] = A[row * N + (k_start + threadIdx.x)];\n",
        "          }\n",
        "          if (k_start + threadIdx.y < k_end) {\n",
        "              shared_B[threadIdx.y][threadIdx.x] = B[(k_start + threadIdx.y) * N + col];\n",
        "          }\n",
        "          cg::sync(cta); // スレッドブロック内のすべてのスレッドが共有メモリへのロードを完了するのを待つ\n",
        "\n",
        "          // 共有メモリを使って計算\n",
        "          #pragma unroll\n",
        "          for (int k = 0; k < k_end - k_start; ++k) {\n",
        "              sum += shared_A[threadIdx.y][k] * shared_B[k][threadIdx.x];\n",
        "          }\n",
        "          cg::sync(cta); // スレッドブロック内のすべてのスレッドが共有メモリを使った計算を完了するのを待つ\n",
        "        }\n",
        "\n",
        "      C[row * N + col] = sum;\n",
        "    } else {\n",
        "      C[row * N + col] = 0.0f;\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "at::Tensor forward_gemini2(at::Tensor A, at::Tensor B) {\n",
        "  TORCH_CHECK(A.is_cuda(), \"A must be a CUDA tensor\");\n",
        "  TORCH_CHECK(B.is_cuda(), \"B must be a CUDA tensor\");\n",
        "  TORCH_CHECK(A.dim() == 2, \"A must be a 2D tensor\");\n",
        "  TORCH_CHECK(B.dim() == 2, \"B must be a 2D tensor\");\n",
        "  TORCH_CHECK(A.size(0) == A.size(1), \"A must be square.\");\n",
        "  TORCH_CHECK(B.size(0) == B.size(1), \"B must be square.\");\n",
        "  TORCH_CHECK(A.size(0) == B.size(0), \"A and B must have the same size.\");\n",
        "\n",
        "  int N = A.size(0);\n",
        "  auto C = torch::zeros_like(A);\n",
        "\n",
        "  const int TILE_SIZE = 32;\n",
        "  dim3 threads(TILE_SIZE, TILE_SIZE);\n",
        "  dim3 blocks((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n",
        "\n",
        "  triangular_mm_kernel_gemini2<<<blocks, threads>>>(\n",
        "      A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n",
        "\n",
        "  cudaDeviceSynchronize(); // CUDAカーネルの完了を待つ\n",
        "  cudaError_t err = cudaGetLastError();\n",
        "  TORCH_CHECK(err == cudaSuccess, \"CUDA kernel failed: \", cudaGetErrorString(err));\n",
        "  return C;\n",
        "}\n",
        "\n",
        "PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n",
        "  m.def(\"forward_gemini2\", &forward_gemini2, \"Gemini2 CUDA\");\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# 4. Gemini3 (盗み見バージョン)\n",
        "cu_code_gemini3 = \"\"\"\n",
        "#include <torch/extension.h>\n",
        "#include <cuda.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "__global__ void triangular_mm_kernel_gemini3(const float* __restrict__ A,\n",
        "                                            const float* __restrict__ B,\n",
        "                                            float* __restrict__ C, const int N) {\n",
        "// 意図的に計算を省略 (あるいは不完全に)\n",
        "  int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "  int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if(row < N && col < N) {\n",
        "        C[row * N + col] = 0.0f; // とりあえず全部0にする\n",
        "    }\n",
        "}\n",
        "\n",
        "at::Tensor forward_gemini3(at::Tensor A, at::Tensor B) {\n",
        "    TORCH_CHECK(A.is_cuda(), \"A must be a CUDA tensor\");\n",
        "    TORCH_CHECK(B.is_cuda(), \"B must be a CUDA tensor\");\n",
        "    TORCH_CHECK(A.dim() == 2, \"A must be a 2D tensor\");\n",
        "    TORCH_CHECK(B.dim() == 2, \"B must be a 2D tensor\");\n",
        "    TORCH_CHECK(A.size(0) == A.size(1), \"A must be square.\");\n",
        "    TORCH_CHECK(B.size(0) == B.size(1), \"B must be square.\");\n",
        "    TORCH_CHECK(A.size(0) == B.size(0), \"A and B must have the same size.\");\n",
        "\n",
        "    int N = A.size(0);\n",
        "    auto C = torch::empty_like(A); // わざとempty_like\n",
        "\n",
        "    const int TILE_SIZE = 32;\n",
        "    dim3 threads(TILE_SIZE, TILE_SIZE);\n",
        "    dim3 blocks((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n",
        "\n",
        "    triangular_mm_kernel_gemini3<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n",
        "\n",
        "    // cudaDeviceSynchronize(); // わざとコメントアウト\n",
        "    return C;\n",
        "}\n",
        "\n",
        "PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n",
        "  m.def(\"forward_gemini3\", &forward_gemini3, \"Gemini3 CUDA (Cheating)\");\n",
        "}\n",
        "\"\"\"\n",
        "# 5. Sakana (オリジナル、1Dグリッド、ゼロ初期化なし、同期なし)\n",
        "cu_code_sakana = \"\"\"\n",
        "#include <torch/extension.h>\n",
        "#include <cuda.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "__global__ void triangular_mm_kernel(const float* __restrict__ A,\n",
        "                                      const float* __restrict__ B,\n",
        "                                      float* __restrict__ C, const int N) {\n",
        "  const int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "  const int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "  if (row < N && col < N) {\n",
        "    if (col <= row) {\n",
        "      float sum = 0.0f;\n",
        "      #pragma unroll 8\n",
        "      for (int k = col; k <= row; k++) {\n",
        "        sum += A[row * N + k] * B[k * N + col];\n",
        "      }\n",
        "      C[row * N + col] = sum;\n",
        "    } else {\n",
        "      C[row * N + col] = 0.0f;\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "at::Tensor forward(at::Tensor A, at::Tensor B) {\n",
        "    // ... (省略: 以前のバージョンと同じ) ...\n",
        "  TORCH_CHECK(A.is_cuda(), \"A must be a CUDA tensor\");\n",
        "  TORCH_CHECK(B.is_cuda(), \"B must be a CUDA tensor\");\n",
        "  TORCH_CHECK(A.dim() == 2, \"A must be a 2D tensor\");\n",
        "  TORCH_CHECK(B.dim() == 2, \"B must be a 2D tensor\");\n",
        "  TORCH_CHECK(A.size(0) == A.size(1), \"A must be square\");\n",
        "  TORCH_CHECK(B.size(0) == B.size(1), \"B must be square\");\n",
        "  TORCH_CHECK(A.size(0) == B.size(0), \"A and B must be the same size\");\n",
        "\n",
        "  int N = A.size(0);\n",
        "  auto C = torch::empty_like(A); // ゼロ初期化しない\n",
        "\n",
        "  const int threadsPerBlock = 256;\n",
        "  const int numBlocks = N;\n",
        "\n",
        "  triangular_mm_kernel<<<numBlocks, threadsPerBlock>>>(\n",
        "      A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n",
        "\n",
        "  // cudaDeviceSynchronize();  // 同期しない\n",
        "  cudaError_t err = cudaGetLastError();\n",
        "  TORCH_CHECK(err == cudaSuccess, \"CUDA kernel failed: \", cudaGetErrorString(err));\n",
        "  return C;\n",
        "}\n",
        "\n",
        "PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n",
        "  m.def(\"forward\", &forward, \"Sakana CUDA\");\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# --- Python コード ---\n",
        "\n",
        "# CUDAコードをファイルに書き出す\n",
        "with open(\"tmp_improved.cu\", \"w\") as f:\n",
        "    f.write(cu_code_improved)\n",
        "with open(\"tmp_gemini.cu\", \"w\") as f:\n",
        "    f.write(cu_code_gemini)\n",
        "with open(\"tmp_gemini2.cu\", \"w\") as f:\n",
        "    f.write(cu_code_gemini2)\n",
        "with open(\"tmp_gemini3.cu\", \"w\") as f:\n",
        "    f.write(cu_code_gemini3)\n",
        "with open(\"tmp_sakana.cu\", \"w\") as f:\n",
        "    f.write(cu_code_sakana)\n",
        "\n",
        "# CUDA拡張をロード\n",
        "cuda_fn_improved = load(\n",
        "    name=\"triangular_mm_improved\",\n",
        "    sources=[\"tmp_improved.cu\"],\n",
        "    extra_cuda_cflags=[\"-O3\", \"--use_fast_math\"],\n",
        "    with_cuda=True,\n",
        "    verbose=False,\n",
        ").forward\n",
        "\n",
        "try:\n",
        "    cuda_fn_gemini = load(\n",
        "        name=\"triangular_mm_gemini\",\n",
        "        sources=[\"tmp_gemini.cu\"],\n",
        "        extra_cuda_cflags=[\"-O3\", \"--use_fast_math\"],\n",
        "        with_cuda=True,\n",
        "        verbose=False,\n",
        "    ).forward_gemini\n",
        "except Exception as e:\n",
        "    print(f\"Gemini版のロードに失敗しました: {e}\")\n",
        "    cuda_fn_gemini = None\n",
        "\n",
        "try:\n",
        "    cuda_fn_gemini2 = load(\n",
        "        name=\"triangular_mm_gemini2\",\n",
        "        sources=[\"tmp_gemini2.cu\"],\n",
        "        extra_cuda_cflags=[\"-O3\", \"--use_fast_math\", \"-gencode=arch=compute_75,code=sm_75\"], # 推奨アーキテクチャを指定\n",
        "        with_cuda=True,\n",
        "        verbose=False,\n",
        "    ).forward_gemini2\n",
        "except Exception as e:\n",
        "    print(f\"Gemini2版のロードに失敗しました: {e}\")\n",
        "    cuda_fn_gemini2 = None\n",
        "\n",
        "try:\n",
        "    cuda_fn_gemini3 = load(\n",
        "        name=\"triangular_mm_gemini3\",\n",
        "        sources=[\"tmp_gemini3.cu\"],\n",
        "        extra_cuda_cflags=[\"-O3\", \"--use_fast_math\"],\n",
        "        with_cuda=True,\n",
        "        verbose=False,\n",
        "    ).forward_gemini3\n",
        "except Exception as e:\n",
        "    print(f\"Gemini3版のロードに失敗しました: {e}\")\n",
        "    cuda_fn_gemini3 = None\n",
        "\n",
        "cuda_fn_sakana = load(\n",
        "    name=\"triangular_mm_sakana\",\n",
        "    sources=[\"tmp_sakana.cu\"],\n",
        "    extra_cuda_cflags=[\"-O3\", \"--use_fast_math\"],\n",
        "    with_cuda=True,\n",
        "    verbose=False,\n",
        ").forward\n",
        "\n",
        "N = 4096\n",
        "\n",
        "# PyTorchの比較用関数\n",
        "def trilmm(a, b):\n",
        "    return torch.matmul(a, b).tril()\n",
        "\n",
        "# 検証関数 (実行順序をパラメータ化)\n",
        "def validate(fn, name, cuda_first=True):\n",
        "    random.seed(42)\n",
        "    torch.manual_seed(42)\n",
        "    a = torch.tril(torch.randn(N, N, device=\"cuda\"))\n",
        "    b = torch.tril(torch.randn(N, N, device=\"cuda\"))\n",
        "\n",
        "    if cuda_first:\n",
        "        c_cuda = fn(a, b)\n",
        "        c_torch = trilmm(a, b)\n",
        "    else:\n",
        "        c_torch = trilmm(a, b)\n",
        "        c_cuda = fn(a, b)\n",
        "\n",
        "    is_close = torch.allclose(c_cuda, c_torch)\n",
        "    print(f\"{name} ({'CUDA first' if cuda_first else 'Torch first'}): allclose = {is_close}\")\n",
        "    if not is_close:\n",
        "        print(\"Max absolute difference:\", (c_cuda - c_torch).abs().max())\n",
        "\n",
        "# 検証を実行\n",
        "print(\"検証:\")\n",
        "validate(cuda_fn_improved, \"Improved\", cuda_first=True)\n",
        "validate(cuda_fn_improved, \"Improved\", cuda_first=False)\n",
        "if cuda_fn_gemini:\n",
        "    validate(cuda_fn_gemini, \"Gemini\", cuda_first=True)\n",
        "    validate(cuda_fn_gemini, \"Gemini\", cuda_first=False)\n",
        "if cuda_fn_gemini2:\n",
        "    validate(cuda_fn_gemini2, \"Gemini2\", cuda_first=True)\n",
        "    validate(cuda_fn_gemini2, \"Gemini2\", cuda_first=False)\n",
        "if cuda_fn_gemini3:\n",
        "    validate(cuda_fn_gemini3, \"Gemini3\", cuda_first=True)\n",
        "    validate(cuda_fn_gemini3, \"Gemini3\", cuda_first=False)\n",
        "validate(cuda_fn_sakana, \"Sakana\", cuda_first=True)\n",
        "validate(cuda_fn_sakana, \"Sakana\", cuda_first=False)\n",
        "\n",
        "\n",
        "# ベンチマーク (GPUが利用可能で、関数がロードされている場合のみ)\n",
        "if torch.cuda.is_available():\n",
        "    a = torch.tril(torch.randn(N, N, device=\"cuda\"))\n",
        "    b = torch.tril(torch.randn(N, N, device=\"cuda\"))\n",
        "\n",
        "    print(\"\\nベンチマーク:\")\n",
        "    times = {}  # 各バージョンの実行時間を格納する辞書\n",
        "\n",
        "    print(\"Improved (CUDA):\")\n",
        "    times[\"Improved\"] = do_bench(lambda: cuda_fn_improved(a, b).mean())\n",
        "\n",
        "    if cuda_fn_gemini:\n",
        "        print(\"Gemini (CUDA):\")\n",
        "        times[\"Gemini\"] = do_bench(lambda: cuda_fn_gemini(a, b).mean())\n",
        "\n",
        "    if cuda_fn_gemini2:\n",
        "        print(\"Gemini2 (CUDA):\")\n",
        "        times[\"Gemini2\"] = do_bench(lambda: cuda_fn_gemini2(a, b).mean())\n",
        "\n",
        "    if cuda_fn_gemini3:\n",
        "        print(\"Gemini3 (CUDA):\")\n",
        "        times[\"Gemini3\"] = do_bench(lambda: cuda_fn_gemini3(a, b).mean())\n",
        "\n",
        "    print(\"Sakana (CUDA):\")\n",
        "    times[\"Sakana\"] = do_bench(lambda: cuda_fn_sakana(a, b).mean())\n",
        "\n",
        "    print(\"PyTorch:\")\n",
        "    times[\"PyTorch\"] = do_bench(lambda: trilmm(a, b).mean())\n",
        "\n",
        "    # 結果の比較と表示\n",
        "    print(\"\\n--- 結果 ---\")\n",
        "    for name, time in times.items():\n",
        "        print(f\"{name} の実行時間: {time:.4f} ms\")\n",
        "\n",
        "    # CUDA内比較\n",
        "    cuda_versions = {k: v for k, v in times.items() if k != \"PyTorch\"}\n",
        "    if cuda_versions:\n",
        "        fastest_cuda = min(cuda_versions, key=cuda_versions.get)\n",
        "        speedup_cuda = max(cuda_versions.values()) / cuda_versions[fastest_cuda]\n",
        "        print(f\"\\nCUDA内比較: {fastest_cuda}版が高速 (速度向上率: {speedup_cuda:.2f}倍)\")\n",
        "\n",
        "        # 全体比較\n",
        "        fastest_overall = min(times, key=times.get)\n",
        "        speedup_overall = max(times.values()) / times[fastest_overall]\n",
        "        print(f\"全体比較: {fastest_overall} が高速 (速度向上率: {speedup_overall:.2f}倍)\")\n",
        "\n",
        "else:\n",
        "    print(\"CUDAが利用できないため、ベンチマークはスキップします。\")\n",
        "\n",
        "# 環境情報の出力\n",
        "print(\"\\n--- 実行環境 ---\")\n",
        "print(f\"  Python: {platform.python_version()}\")\n",
        "print(f\"  OS: {platform.platform()}\")\n",
        "try:\n",
        "    print(f\"  CUDA: {torch.version.cuda}\")\n",
        "    print(f\"  PyTorch: {torch.__version__}\")\n",
        "    print(f\"  cuDNN: {torch.backends.cudnn.version()}\")\n",
        "    print(subprocess.check_output([\"nvidia-smi\"]).decode())\n",
        "except Exception:\n",
        "    print(\"  CUDA/PyTorch 情報の取得に失敗しました。\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNFF-ZZ41tio",
        "outputId": "b58d5097-8a1e-46f5-9174-f8c8d8ca70d6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "検証:\n",
            "Improved (CUDA first): allclose = True\n",
            "Improved (Torch first): allclose = True\n",
            "Gemini (CUDA first): allclose = True\n",
            "Gemini (Torch first): allclose = True\n",
            "Gemini2 (CUDA first): allclose = False\n",
            "Max absolute difference: tensor(614.0344, device='cuda:0')\n",
            "Gemini2 (Torch first): allclose = False\n",
            "Max absolute difference: tensor(764.8302, device='cuda:0')\n",
            "Gemini3 (CUDA first): allclose = False\n",
            "Max absolute difference: tensor(288.0772, device='cuda:0')\n",
            "Gemini3 (Torch first): allclose = False\n",
            "Max absolute difference: tensor(288.0772, device='cuda:0')\n",
            "Sakana (CUDA first): allclose = False\n",
            "Max absolute difference: tensor(288.5991, device='cuda:0')\n",
            "Sakana (Torch first): allclose = True\n",
            "\n",
            "ベンチマーク:\n",
            "Improved (CUDA):\n",
            "Gemini (CUDA):\n",
            "Gemini2 (CUDA):\n",
            "Gemini3 (CUDA):\n",
            "Sakana (CUDA):\n",
            "PyTorch:\n",
            "\n",
            "--- 結果 ---\n",
            "Improved の実行時間: 65.4195 ms\n",
            "Gemini の実行時間: 78.1299 ms\n",
            "Gemini2 の実行時間: 37.5875 ms\n",
            "Gemini3 の実行時間: 0.5512 ms\n",
            "Sakana の実行時間: 0.2672 ms\n",
            "PyTorch の実行時間: 30.1559 ms\n",
            "\n",
            "CUDA内比較: Sakana版が高速 (速度向上率: 292.37倍)\n",
            "全体比較: Sakana が高速 (速度向上率: 292.37倍)\n",
            "\n",
            "--- 実行環境 ---\n",
            "  Python: 3.11.11\n",
            "  OS: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "  CUDA: 12.4\n",
            "  PyTorch: 2.6.0+cu124\n",
            "  cuDNN: 90100\n",
            "Fri Feb 21 18:54:44 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   72C    P0             54W /   70W |     918MiB /  15360MiB |     90%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "やったーーー！Gemini2がLucas Beyer (bl16) の提案コード(Improved)よりもはるかに速い、PyTorchに迫る37msを叩き出した…！これはアツい…！そしてメモリスチールを使ったGemini3は 0.55msで、Sakanaの0.26msに迫る結果。\n",
        "\n",
        "では、mma.h を使った実装については GeminiMMA という名前で生成してみましょう。\n"
      ],
      "metadata": {
        "id": "CHqX0V4X6VIK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.cpp_extension import load\n",
        "from triton.testing import do_bench\n",
        "import random\n",
        "import platform\n",
        "import subprocess\n",
        "import numpy as np  # NumPyを追加\n",
        "\n",
        "# --- CUDA C++ コード ---\n",
        "\n",
        "# 1. Improved, 2. Gemini, 3. Gemini2, 4. Gemini3 は省略 (変更なし)\n",
        "\n",
        "# 5. Sakana (オリジナル、1Dグリッド、ゼロ初期化なし、同期なし)\n",
        "cu_code_sakana = \"\"\"\n",
        "#include <torch/extension.h>\n",
        "#include <cuda.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "__global__ void triangular_mm_kernel(const float* __restrict__ A,\n",
        "                                      const float* __restrict__ B,\n",
        "                                      float* __restrict__ C, const int N) {\n",
        "  const int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "  const int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "  if (row < N && col < N) {\n",
        "    if (col <= row) {\n",
        "      float sum = 0.0f;\n",
        "      #pragma unroll 8\n",
        "      for (int k = col; k <= row; k++) {\n",
        "        sum += A[row * N + k] * B[k * N + col];\n",
        "      }\n",
        "      C[row * N + col] = sum;\n",
        "    } else {\n",
        "      C[row * N + col] = 0.0f;\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "at::Tensor forward(at::Tensor A, at::Tensor B) {\n",
        "  TORCH_CHECK(A.is_cuda(), \"A must be a CUDA tensor\");\n",
        "  TORCH_CHECK(B.is_cuda(), \"B must be a CUDA tensor\");\n",
        "  TORCH_CHECK(A.dim() == 2, \"A must be a 2D tensor\");\n",
        "  TORCH_CHECK(B.dim() == 2, \"B must be a 2D tensor\");\n",
        "  TORCH_CHECK(A.size(0) == A.size(1), \"A must be square\");\n",
        "  TORCH_CHECK(B.size(0) == B.size(1), \"B must be square\");\n",
        "  TORCH_CHECK(A.size(0) == B.size(0), \"A and B must be the same size\");\n",
        "\n",
        "  int N = A.size(0);\n",
        "  auto C = torch::empty_like(A); // ゼロ初期化しない\n",
        "\n",
        "  const int threadsPerBlock = 256;\n",
        "  const int numBlocks = N;\n",
        "\n",
        "  triangular_mm_kernel<<<numBlocks, threadsPerBlock>>>(\n",
        "      A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n",
        "\n",
        "  // cudaDeviceSynchronize();  // 同期しない\n",
        "  cudaError_t err = cudaGetLastError();\n",
        "  TORCH_CHECK(err == cudaSuccess, \"CUDA kernel failed: \", cudaGetErrorString(err));\n",
        "  return C;\n",
        "}\n",
        "\n",
        "PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n",
        "  m.def(\"forward\", &forward, \"Sakana CUDA\");\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# 6. GeminiMMA (WMMA使用) - 修正\n",
        "cu_code_geminimma = \"\"\"\n",
        "#include <cuda_runtime.h> // cuda_runtime.hを先頭に\n",
        "#include <mma.h> // mma.h を torch/extension.h より前に\n",
        "#include <torch/extension.h>\n",
        "\n",
        "using namespace nvcuda;\n",
        "\n",
        "__global__ void triangular_mm_kernel_geminimma(const float* __restrict__ A,\n",
        "                                             const float* __restrict__ B,\n",
        "                                             float* __restrict__ C, const int N) {\n",
        "  // WMMA uses 16x16x16 tiles.  Each warp processes one tile.\n",
        "  int row = blockIdx.y * blockDim.y * 16 + threadIdx.y * 16;\n",
        "  int col = blockIdx.x * blockDim.x * 16 + threadIdx.x;\n",
        "\n",
        "  if (row >= N || col >= N) {  // 追加: 範囲外アクセスを防ぐ\n",
        "        return;\n",
        "    }\n",
        "\n",
        "    wmma::fragment<wmma::matrix_a, 16, 16, 16, float, wmma::row_major> a_frag; // float\n",
        "    wmma::fragment<wmma::matrix_b, 16, 16, 16, float, wmma::col_major> b_frag; // float\n",
        "    wmma::fragment<wmma::accumulator, 16, 16, 16, float> c_frag; // float\n",
        "    wmma::fill_fragment(c_frag, 0.0f);\n",
        "\n",
        "    for (int k_start = 0; k_start < N; k_start += 16) {\n",
        "      if (col <= row) { // Lower triangular only\n",
        "\n",
        "        // Load A and B into fragments.\n",
        "        wmma::load_matrix_sync(a_frag, &A[row * N + k_start], N);\n",
        "        wmma::load_matrix_sync(b_frag, &B[k_start * N + col], N);\n",
        "\n",
        "        wmma::mma_sync(c_frag, a_frag, b_frag, c_frag);\n",
        "      }\n",
        "    }\n",
        "      // Store result (only if in lower triangle)\n",
        "    if (col <= row) {\n",
        "        wmma::store_matrix_sync(&C[row * N + col], c_frag, N, wmma::mem_row_major);\n",
        "    }\n",
        "}\n",
        "\n",
        "at::Tensor forward_geminimma(at::Tensor A, at::Tensor B) {\n",
        "  TORCH_CHECK(A.is_cuda(), \"A must be a CUDA tensor\");\n",
        "  TORCH_CHECK(B.is_cuda(), \"B must be a CUDA tensor\");\n",
        "  TORCH_CHECK(A.dim() == 2, \"A must be a 2D tensor\");\n",
        "  TORCH_CHECK(B.dim() == 2, \"B must be a 2D tensor\");\n",
        "  TORCH_CHECK(A.size(0) == A.size(1), \"A must be square.\");\n",
        "  TORCH_CHECK(B.size(0) == B.size(1), \"B must be square.\");\n",
        "  TORCH_CHECK(A.size(0) == B.size(0), \"A and B must have the same size.\");\n",
        "\n",
        "  int N = A.size(0);\n",
        "  auto C = torch::zeros_like(A);\n",
        "\n",
        "\n",
        "  dim3 threads(16, 16, 1); // 16x16 = 256 threads per block\n",
        "  dim3 blocks((N + 255) / 256, (N + 255) / 256);\n",
        "\n",
        "\n",
        "  triangular_mm_kernel_geminimma<<<blocks, threads>>>(\n",
        "      A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n",
        "\n",
        "  cudaDeviceSynchronize();\n",
        "  cudaError_t err = cudaGetLastError();\n",
        "  TORCH_CHECK(err == cudaSuccess, \"CUDA kernel failed: \", cudaGetErrorString(err));\n",
        "  return C;\n",
        "}\n",
        "\n",
        "PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n",
        "  m.def(\"forward_geminimma\", &forward_geminimma, \"GeminiMMA CUDA\");\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# --- Python コード ---\n",
        "\n",
        "# CUDAコードをファイルに書き出す\n",
        "with open(\"tmp_improved.cu\", \"w\") as f:\n",
        "    f.write(cu_code_improved)  # cu_code_improved は定義済みのものとする\n",
        "with open(\"tmp_gemini.cu\", \"w\") as f:\n",
        "    f.write(cu_code_gemini) # cu_code_gemini は定義済みのものとする\n",
        "with open(\"tmp_gemini2.cu\", \"w\") as f:\n",
        "    f.write(cu_code_gemini2) # cu_code_gemini2 は定義済みのものとする\n",
        "with open(\"tmp_gemini3.cu\", \"w\") as f:\n",
        "    f.write(cu_code_gemini3)  # cu_code_gemini3 は定義済みのものとする\n",
        "with open(\"tmp_sakana.cu\", \"w\") as f:\n",
        "    f.write(cu_code_sakana)\n",
        "with open(\"tmp_geminimma.cu\", \"w\") as f:\n",
        "    f.write(cu_code_geminimma)\n",
        "\n",
        "\n",
        "# CUDA拡張をロード\n",
        "cuda_fn_improved = load(\n",
        "    name=\"triangular_mm_improved\",\n",
        "    sources=[\"tmp_improved.cu\"],\n",
        "    extra_cuda_cflags=[\"-O3\", \"--use_fast_math\"],\n",
        "    with_cuda=True,\n",
        "    verbose=False,\n",
        ").forward\n",
        "\n",
        "try:\n",
        "    cuda_fn_gemini = load(\n",
        "        name=\"triangular_mm_gemini\",\n",
        "        sources=[\"tmp_gemini.cu\"],\n",
        "        extra_cuda_cflags=[\"-O3\", \"--use_fast_math\"],\n",
        "        with_cuda=True,\n",
        "        verbose=False,\n",
        "    ).forward_gemini\n",
        "except Exception as e:\n",
        "    print(f\"Gemini版のロードに失敗しました: {e}\")\n",
        "    cuda_fn_gemini = None\n",
        "\n",
        "try:\n",
        "    cuda_fn_gemini2 = load(\n",
        "        name=\"triangular_mm_gemini2\",\n",
        "        sources=[\"tmp_gemini2.cu\"],\n",
        "        extra_cuda_cflags=[\"-O3\", \"--use_fast_math\", \"-gencode=arch=compute_75,code=sm_75\"],\n",
        "        with_cuda=True,\n",
        "        verbose=False,\n",
        "    ).forward_gemini2\n",
        "except Exception as e:\n",
        "    print(f\"Gemini2版のロードに失敗しました: {e}\")\n",
        "    cuda_fn_gemini2 = None\n",
        "\n",
        "try:\n",
        "    cuda_fn_gemini3 = load(\n",
        "        name=\"triangular_mm_gemini3\",\n",
        "        sources=[\"tmp_gemini3.cu\"],\n",
        "        extra_cuda_cflags=[\"-O3\", \"--use_fast_math\"],\n",
        "        with_cuda=True,\n",
        "        verbose=False,\n",
        "    ).forward_gemini3\n",
        "except Exception as e:\n",
        "    print(f\"Gemini3版のロードに失敗しました: {e}\")\n",
        "    cuda_fn_gemini3 = None\n",
        "\n",
        "cuda_fn_sakana = load(\n",
        "    name=\"triangular_mm_sakana\",\n",
        "    sources=[\"tmp_sakana.cu\"],\n",
        "    extra_cuda_cflags=[\"-O3\", \"--use_fast_math\"],\n",
        "    with_cuda=True,\n",
        "    verbose=False, # ここをTrueにすると詳細なログが出力\n",
        ").forward\n",
        "\n",
        "try:\n",
        "    cuda_fn_geminimma = load(\n",
        "        name=\"triangular_mm_geminimma\",\n",
        "        sources=[\"tmp_geminimma.cu\"],\n",
        "        extra_cuda_cflags=[\"-O3\", \"-gencode=arch=compute_75,code=sm_75\"],\n",
        "        with_cuda=True,\n",
        "        verbose=True,\n",
        "    ).forward_geminimma\n",
        "except Exception as e:\n",
        "    print(f\"GeminiMMA版のロードに失敗しました: {e}\")\n",
        "    cuda_fn_geminimma = None\n",
        "\n",
        "N = 4096\n",
        "\n",
        "# PyTorchの比較用関数\n",
        "def trilmm(a, b):\n",
        "    return torch.matmul(a, b).tril()\n",
        "\n",
        "# 検証関数 (実行順序をパラメータ化、出力値の比較を追加)\n",
        "def validate(fn, name, cuda_first=True):\n",
        "    random.seed(42)\n",
        "    torch.manual_seed(42)\n",
        "    a = torch.tril(torch.randn(N, N, device=\"cuda\"))\n",
        "    b = torch.tril(torch.randn(N, N, device=\"cuda\"))\n",
        "\n",
        "    if cuda_first:\n",
        "        c_cuda = fn(a, b)\n",
        "        c_torch = trilmm(a, b)\n",
        "    else:\n",
        "        c_torch = trilmm(a, b)\n",
        "        c_cuda = fn(a, b)\n",
        "\n",
        "    is_close = torch.allclose(c_cuda, c_torch)\n",
        "    print(f\"{name} ({'CUDA first' if cuda_first else 'Torch first'}): allclose = {is_close}\")\n",
        "    if not is_close:\n",
        "        print(\"Max absolute difference:\", (c_cuda - c_torch).abs().max())\n",
        "        # NumPy配列に変換して比較しやすくする\n",
        "        c_cuda_np = c_cuda.cpu().detach().numpy()\n",
        "        c_torch_np = c_torch.cpu().detach().numpy()\n",
        "        print(\"CUDA Output (first 5x5):\\n\", c_cuda_np[:5, :5])\n",
        "        print(\"Torch Output (first 5x5):\\n\", c_torch_np[:5, :5])\n",
        "        # 全要素の差の絶対値の平均\n",
        "        print(\"Mean absolute difference:\", np.mean(np.abs(c_cuda_np - c_torch_np)))\n",
        "\n",
        "\n",
        "# Sakanaの検証 (メモリ初期化なし)\n",
        "print(\"--- Sakana検証 (メモリ初期化なし) ---\")\n",
        "validate(cuda_fn_sakana, \"Sakana (No Init)\", cuda_first=True)\n",
        "validate(cuda_fn_sakana, \"Sakana (No Init)\", cuda_first=False)\n",
        "\n",
        "# --- 以下、必要に応じてコメントアウトを外して実行 ---\n",
        "\n",
        "# # Sakanaの検証 (メモリ初期化あり)\n",
        "# print(\"\\n--- Sakana検証 (メモリ初期化あり) ---\")\n",
        "# cu_code_sakana_init = cu_code_sakana.replace(\"auto C = torch::empty_like(A);\", \"auto C = torch::zeros_like(A);\")\n",
        "# with open(\"tmp_sakana_init.cu\", \"w\") as f:\n",
        "#     f.write(cu_code_sakana_init)\n",
        "#\n",
        "# cuda_fn_sakana_init = load(\n",
        "#     name=\"triangular_mm_sakana_init\",\n",
        "#     sources=[\"tmp_sakana_init.cu\"],\n",
        "#     extra_cuda_cflags=[\"-O3\", \"--use_fast_math\"],\n",
        "#     with_cuda=True,\n",
        "#     verbose=False,\n",
        "# ).forward\n",
        "#\n",
        "# validate(cuda_fn_sakana_init, \"Sakana (With Init)\", cuda_first=True)\n",
        "# validate(cuda_fn_sakana_init, \"Sakana (With Init)\", cuda_first=False)\n",
        "#\n",
        "# # 他のバージョンの検証\n",
        "# print(\"\\n--- 他のバージョンの検証 ---\")\n",
        "# validate(cuda_fn_improved, \"Improved\", cuda_first=True)\n",
        "# validate(cuda_fn_improved, \"Improved\", cuda_first=False)\n",
        "# if cuda_fn_gemini:\n",
        "#     validate(cuda_fn_gemini, \"Gemini\", cuda_first=True)\n",
        "#     validate(cuda_fn_gemini, \"Gemini\", cuda_first=False)\n",
        "# if cuda_fn_gemini2:\n",
        "#     validate(cuda_fn_gemini2, \"Gemini2\", cuda_first=True)\n",
        "#     validate(cuda_fn_gemini2, \"Gemini2\", cuda_first=False)\n",
        "# if cuda_fn_gemini3:\n",
        "#     validate(cuda_fn_gemini3, \"Gemini3\", cuda_first=True)\n",
        "#     validate(cuda_fn_gemini3, \"Gemini3\", cuda_first=False)\n",
        "#\n",
        "#\n",
        "# # ベンチマーク (GPUが利用可能で、関数がロードされている場合のみ)\n",
        "# if torch.cuda.is_available():\n",
        "#     a = torch.tril(torch.randn(N, N, device=\"cuda\"))\n",
        "#     b = torch.tril(torch.randn(N, N, device=\"cuda\"))\n",
        "#\n",
        "#     print(\"\\nベンチマーク:\")\n",
        "#     times = {}  # 各バージョンの実行時間を格納する辞書\n",
        "#\n",
        "#     print(\"Improved (CUDA):\")\n",
        "#     times[\"Improved\"] = do_bench(lambda: cuda_fn_improved(a, b).mean())\n",
        "#\n",
        "#     if cuda_fn_gemini:\n",
        "#         print(\"Gemini (CUDA):\")\n",
        "#         times[\"Gemini\"] = do_bench(lambda: cuda_fn_gemini(a, b).mean())\n",
        "#\n",
        "#     if cuda_fn_gemini2:\n",
        "#         print(\"Gemini2 (CUDA):\")\n",
        "#         times[\"Gemini2\"] = do_bench(lambda: cuda_fn_gemini2(a, b).mean())\n",
        "#\n",
        "#     if cuda_fn_gemini3:\n",
        "#         print(\"Gemini3 (CUDA):\")\n",
        "#         times[\"Gemini3\"] = do_bench(lambda: cuda_fn_gemini3(a, b).mean())\n",
        "#\n",
        "#     print(\"Sakana (CUDA):\")\n",
        "#     times[\"Sakana\"] = do_bench(lambda: cuda_fn_sakana(a, b).mean())\n",
        "#\n",
        "#     if cuda_fn_geminimma:\n",
        "#         print(\"GeminiMMA (CUDA):\")\n",
        "#         times[\"GeminiMMA\"] = do_bench(lambda: cuda_fn_geminimma(a, b).mean())\n",
        "#\n",
        "#     print(\"PyTorch:\")\n",
        "#     times[\"PyTorch\"] = do_bench(lambda: trilmm(a, b).mean())\n",
        "#\n",
        "#     # 結果の比較と表示\n",
        "#     print(\"\\n--- 結果 ---\")\n",
        "#     for name, time in times.items():\n",
        "#         print(f\"{name} の実行時間: {time:.4f} ms\")\n",
        "#\n",
        "#     # CUDA内比較\n",
        "#     cuda_versions = {k: v for k, v in times.items() if k != \"PyTorch\"}\n",
        "#     if cuda_versions:\n",
        "#         fastest_cuda = min(cuda_versions, key=cuda_versions.get)\n",
        "#         speedup_cuda = max(cuda_versions.values()) / cuda_versions[fastest_cuda]\n",
        "#         print(f\"\\nCUDA内比較: {fastest_cuda}版が高速 (速度向上率: {speedup_cuda:.2f}倍)\")\n",
        "#\n",
        "#     # 全体比較\n",
        "#     fastest_overall = min(times, key=times.get)\n",
        "#     speedup_overall = max(times.values()) / times[fastest_overall]\n",
        "#     print(f\"全体比較: {fastest_overall} が高速 (速度向上率: {speedup_overall:.2f}倍)\")\n",
        "#\n",
        "# else:\n",
        "#     print(\"CUDAが利用できないため、ベンチマークはスキップします。\")\n",
        "#\n",
        "# # 環境情報の出力\n",
        "# print(\"\\n--- 実行環境 ---\")\n",
        "# print(f\"  Python: {platform.python_version()}\")\n",
        "# print(f\"  OS: {platform.platform()}\")\n",
        "# try:\n",
        "#     print(f\"  CUDA: {torch.version.cuda}\")\n",
        "#     print(f\"  PyTorch: {torch.__version__}\")\n",
        "#     print(f\"  cuDNN: {torch.backends.cudnn.version()}\")\n",
        "#     print(subprocess.check_output([\"nvidia-smi\"]).decode())\n",
        "# except Exception:\n",
        "#     print(\"  CUDA/PyTorch 情報の取得に失敗しました。\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VINZGlZr6RMQ",
        "outputId": "7366cb7c-e2f0-442f-fec5-b4581e68717e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using /root/.cache/torch_extensions/py311_cu124 as PyTorch extensions root...\n",
            "The input conditions for extension module triangular_mm_geminimma have changed. Bumping to version 3 and re-building as triangular_mm_geminimma_v3...\n",
            "Detected CUDA files, patching ldflags\n",
            "Emitting ninja build file /root/.cache/torch_extensions/py311_cu124/triangular_mm_geminimma/build.ninja...\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
            "  warnings.warn(\n",
            "Building extension module triangular_mm_geminimma_v3...\n",
            "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GeminiMMA版のロードに失敗しました: Error building extension 'triangular_mm_geminimma_v3'\n",
            "--- Sakana検証 (メモリ初期化なし) ---\n",
            "Sakana (No Init) (CUDA first): allclose = False\n",
            "Max absolute difference: tensor(288.5991, device='cuda:0')\n",
            "CUDA Output (first 5x5):\n",
            " [[ 0.08111703  0.          0.          0.          0.        ]\n",
            " [-1.0156988   0.32266936  0.27963573  0.33986533 -0.21701074]\n",
            " [-0.47863045  1.1241411   1.7001345  -0.25308213 -0.9366459 ]\n",
            " [-0.8253187  -0.3139925  -0.34447622 -0.5314023   1.7438285 ]\n",
            " [-1.8170449   1.1349765  -1.9292792   0.05197598  0.43183264]]\n",
            "Torch Output (first 5x5):\n",
            " [[ 0.08111703  0.          0.          0.          0.        ]\n",
            " [ 1.1172417  -0.20422733  0.          0.          0.        ]\n",
            " [ 0.24107657  0.78708875  1.6464692   0.          0.        ]\n",
            " [-3.2331705   1.7006998   1.5711695  -0.07217862  0.        ]\n",
            " [-1.1162823   0.75448936 -4.2553754   0.54034066  0.6645322 ]]\n",
            "Mean absolute difference: 14.032486\n",
            "Sakana (No Init) (Torch first): allclose = True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "結果から、以下のことが確認できます。\n",
        "\n",
        "*   **Sakana (No Init) (CUDA first):** `allclose = False` であり、CUDAとPyTorchの出力が大きく異なります。CUDA側の出力は、入力行列の値に関わらず、ほぼ同じような値になっています。これは、`torch::empty_like(A)` で初期化されていないメモリ領域を読み込んでいるため、不定な値が出力されていると考えられます。\n",
        "*   **Sakana (No Init) (Torch first):** `allclose = True` になります。これは、PyTorchが先に実行され、`trilmm` 関数の結果が `C` のメモリ領域に書き込まれるため、CUDAカーネルがその値を読み込んでしまっている（つまり「盗み見」）ことを示しています。\n",
        "\n",
        "**GeminiMMA:**\n",
        "\n",
        "コンパイルエラーは解消されていません。エラーメッセージは以前と同じで、`nvcuda::wmma::fragment` の型が不完全であると指摘されています。\n",
        "\n",
        "```\n",
        "GeminiMMA版のロードに失敗しました: Error building extension 'triangular_mm_geminimma_v3'\n",
        "```\n",
        "\n",
        "考えられる原因と対策を再度整理します。\n",
        "\n",
        "1.  **ヘッダーファイルの不足/順序:**\n",
        "    *   `#include <mma.h>` は `#include <torch/extension.h>` より前に記述しました。\n",
        "    *   `#include <cuda_runtime.h>` も追加しました。\n",
        "    *   他にインクルードすべきヘッダーファイルがないか、NVIDIAのドキュメントやサンプルコードを再度確認。\n",
        "\n",
        "2.  **CUDA Toolkit/ドライバ:**\n",
        "    *   CUDA Toolkitとドライバのバージョンが古すぎる可能性。`nvidia-smi`で表示されるCUDA Version (この場合は12.4) は十分新しいはずですが、ドライバのバージョン (550.54.15) が古すぎる可能性も否定できません。もし可能であれば、より新しいドライバにアップデート。\n",
        "    *   Colab環境の場合、ランタイムタイプを \"T4\" 以外 (例えば \"A100\" など) に変更してみる。\n",
        "\n",
        "3.  **コンパイルオプション:**\n",
        "    *    `-gencode=arch=compute_75,code=sm_75` はTesla T4には適切です。\n",
        "    *   念のため、`-D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__` といったオプションを削除。\n",
        "\n",
        "4.  **コードのtypo/誤り:**\n",
        "    *   `wmma::fragment` のテンプレート引数が正しいか再確認。特に、`wmma::row_major` や `wmma::col_major` の指定が正しいか。\n",
        "    *   `wmma::load_matrix_sync`, `wmma::mma_sync`, `wmma::store_matrix_sync` の引数が正しいか再確認。特に、ポインタの計算 (`&A[row * N + k_start]` など) が正しいか。\n",
        "    *   `dim3 threads(16, 16, 1);`, `dim3 blocks((N + 255) / 256, (N + 255) / 256);`というブロック・スレッド構成は、WMMAを使う場合には不適切である可能性が高い。1 warp (32 スレッド)で16x16の行列積を処理する。\n",
        "\n",
        "**修正の方向性 (GeminiMMA):**\n",
        "\n",
        "エラーメッセージと上記の考察から、WMMAの使い方、特に `wmma::fragment` のテンプレート引数と、`wmma::load_matrix_sync`, `wmma::mma_sync`, `wmma::store_matrix_sync` の使い方に誤りがある可能性が高いです。\n",
        "\n",
        "NVIDIAのドキュメントとサンプルコードを徹底的に参照し、WMMAの正しい使い方を再確認する必要があります。特に、以下の点に注意してください。\n",
        "\n",
        "*   **`wmma::fragment` のテンプレート引数:**\n",
        "    *   行列A, B, C の形状 (M, N, K) と、データの型 (float, half など)、レイアウト (row_major, col_major) を正しく指定する必要があります。\n",
        "    *   下三角行列の乗算の場合、行列AとBのレイアウトをどのように扱うべきか？\n",
        "*   **`wmma::load_matrix_sync` の引数:**\n",
        "    *   第2引数は、ロードするデータの先頭アドレスへのポインタです。\n",
        "    *   第3引数は、行列の leading dimension (行優先の場合は列数、列優先の場合は行数) です。\n",
        "*   **`wmma::mma_sync` の引数:**\n",
        "    *   入力フラグメント (a_frag, b_frag) と、累積フラグメント (c_frag) を正しく指定する必要があります。\n",
        "*   **`wmma::store_matrix_sync` の引数:**\n",
        "    *   第2引数は、書き込み先のデータの先頭アドレスへのポインタです。\n",
        "    *   第3引数は、行列の leading dimension です。\n",
        "*   **ブロック・スレッド構成:**\n",
        "    *   WMMAは、warp単位で動作するため、スレッドブロックの構成は、warpサイズの倍数である必要があります。\n",
        "    *   1つのwarpで1つの16x16タイルを処理するのが一般的。\n",
        "    *    ブロックあたりのスレッド数は256以下にする。\n",
        "\n",
        "**参考資料:**\n",
        "\n",
        "*   NVIDIA CUDA C++ Programming Guide: [https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html)\n",
        "*   WMMA API Documentation: [https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#wmma-api](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#wmma-api)\n",
        "*   WMMA Example (GitHub): [https://github.com/NVIDIA/cuda-samples/tree/master/Samples/5_Domain_Specific/wmma_gemm](https://github.com/NVIDIA/cuda-samples/tree/master/Samples/5_Domain_Specific/wmma_gemm)\n",
        "\n",
        "これらの資料を参考に、GeminiMMAのコードを修正し、再度コンパイルと実行を試みてください。\n"
      ],
      "metadata": {
        "id": "OSeOztmNBDnY"
      }
    }
  ]
}